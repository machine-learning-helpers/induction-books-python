{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# References\n",
       "* https://blog.keras.io/building-autoencoders-in-keras.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# References\n",
    "* https://blog.keras.io/building-autoencoders-in-keras.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.3572 - val_loss: 0.2720\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2662 - val_loss: 0.2567\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2461 - val_loss: 0.2326\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2241 - val_loss: 0.2133\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2077 - val_loss: 0.1997\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1961 - val_loss: 0.1899\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.1876 - val_loss: 0.1826\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1810 - val_loss: 0.1766\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1754 - val_loss: 0.1716\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.1707 - val_loss: 0.1673\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.1665 - val_loss: 0.1632\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.1627 - val_loss: 0.1595\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.1592 - val_loss: 0.1561\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.1559 - val_loss: 0.1530\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.1529 - val_loss: 0.1501\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.1501 - val_loss: 0.1473\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1474 - val_loss: 0.1448\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1448 - val_loss: 0.1421\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.1423 - val_loss: 0.1398\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.1400 - val_loss: 0.1374\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.1377 - val_loss: 0.1352\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1355 - val_loss: 0.1331\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.1334 - val_loss: 0.1309\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.1314 - val_loss: 0.1289\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.1294 - val_loss: 0.1270\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1276 - val_loss: 0.1251\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.1258 - val_loss: 0.1234\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.1241 - val_loss: 0.1218\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.1225 - val_loss: 0.1201\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.1209 - val_loss: 0.1187\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1195 - val_loss: 0.1173\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.1182 - val_loss: 0.1160\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.1170 - val_loss: 0.1148\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1159 - val_loss: 0.1138\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.1149 - val_loss: 0.1128\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.1140 - val_loss: 0.1119\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.1131 - val_loss: 0.1111\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.1124 - val_loss: 0.1103\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1117 - val_loss: 0.1097\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.1110 - val_loss: 0.1091\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.1105 - val_loss: 0.1085\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1099 - val_loss: 0.1080\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.1094 - val_loss: 0.1075\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.1089 - val_loss: 0.1070\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1085 - val_loss: 0.1066\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.1081 - val_loss: 0.1061\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.1077 - val_loss: 0.1058\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.1073 - val_loss: 0.1054\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1069 - val_loss: 0.1051\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1066 - val_loss: 0.1047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76ccf0a1d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76c95c2cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "# add a Dense layer with a L1 activity regularizer\n",
    "encoded = Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.3498 - val_loss: 0.2638\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2585 - val_loss: 0.2539\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2445 - val_loss: 0.2331\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.2247 - val_loss: 0.2160\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.2076 - val_loss: 0.1989\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.1941 - val_loss: 0.1887\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.1854 - val_loss: 0.1780\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.1794 - val_loss: 0.1748\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.1725 - val_loss: 0.1753\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.1706 - val_loss: 0.1649\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.3671 - val_loss: 0.6744\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.6531 - val_loss: 0.6323\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.6133 - val_loss: 0.5951\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.5786 - val_loss: 0.5629\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.5486 - val_loss: 0.5350\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.5225 - val_loss: 0.5108\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.4999 - val_loss: 0.4896\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.4801 - val_loss: 0.4711\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.4627 - val_loss: 0.4548\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.4473 - val_loss: 0.4404\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4337 - val_loss: 0.4276\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.4217 - val_loss: 0.4162\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.4109 - val_loss: 0.4060\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.4013 - val_loss: 0.3969\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.3926 - val_loss: 0.3887\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.3848 - val_loss: 0.3813\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.3777 - val_loss: 0.3745\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.3713 - val_loss: 0.3684\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.3654 - val_loss: 0.3628\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3601 - val_loss: 0.3577\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.3552 - val_loss: 0.3531\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3507 - val_loss: 0.3487\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.3466 - val_loss: 0.3448\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.3428 - val_loss: 0.3411\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.3392 - val_loss: 0.3377\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3360 - val_loss: 0.3346\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.3329 - val_loss: 0.3316\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.3301 - val_loss: 0.3289\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.3275 - val_loss: 0.3264\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.3250 - val_loss: 0.3240\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.3227 - val_loss: 0.3217\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3205 - val_loss: 0.3197\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3185 - val_loss: 0.3177\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.3166 - val_loss: 0.3158\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3148 - val_loss: 0.3141\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.3131 - val_loss: 0.3124\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.3115 - val_loss: 0.3109\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.3100 - val_loss: 0.3094\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.3086 - val_loss: 0.3080\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.3072 - val_loss: 0.3067\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.3060 - val_loss: 0.3054\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.3047 - val_loss: 0.3043\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.3036 - val_loss: 0.3031\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.3025 - val_loss: 0.3020\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.3014 - val_loss: 0.3010\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.3004 - val_loss: 0.3000\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.2995 - val_loss: 0.2991\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.2985 - val_loss: 0.2982\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.2977 - val_loss: 0.2973\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.2968 - val_loss: 0.2965\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.2960 - val_loss: 0.2957\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.2953 - val_loss: 0.2950\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.2945 - val_loss: 0.2942\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2938 - val_loss: 0.2935\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.2931 - val_loss: 0.2929\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.2925 - val_loss: 0.2922\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.2918 - val_loss: 0.2916\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.2912 - val_loss: 0.2910\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.2907 - val_loss: 0.2904\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2901 - val_loss: 0.2899\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.2896 - val_loss: 0.2893\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2890 - val_loss: 0.2888\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.2885 - val_loss: 0.2883\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2880 - val_loss: 0.2878\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2876 - val_loss: 0.2874\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2871 - val_loss: 0.2869\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2867 - val_loss: 0.2865\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.2863 - val_loss: 0.2861\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.2858 - val_loss: 0.2857\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2854 - val_loss: 0.2853\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.2851 - val_loss: 0.2849\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.2847 - val_loss: 0.2845\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.2843 - val_loss: 0.2841\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.2840 - val_loss: 0.2838\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.2836 - val_loss: 0.2835\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.2833 - val_loss: 0.2831\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.2830 - val_loss: 0.2828\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2826 - val_loss: 0.2825\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.2823 - val_loss: 0.2822\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.2820 - val_loss: 0.2819\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2818 - val_loss: 0.2816\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.2815 - val_loss: 0.2813\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.2812 - val_loss: 0.2810\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.2809 - val_loss: 0.2808\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.2807 - val_loss: 0.2805\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.2804 - val_loss: 0.2803\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.2802 - val_loss: 0.2800\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2799 - val_loss: 0.2798\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.2797 - val_loss: 0.2796\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.2795 - val_loss: 0.2793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76c9a41518>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "# adapt this if using `channels_first` image data format\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "# adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "TensorBoard 1.6.0 at http://aef8459a755d:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=/tmp/autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (7, 7, 32)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "encoded = LSTM(latent_dim)(inputs)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end autoencoder\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]]) * epsilon_std\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
